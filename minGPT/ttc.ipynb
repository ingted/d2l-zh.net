{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_historical_data(symbol='BTC/USDT', timeframe='1h', limit=10000):\n",
    "    all_candles = []\n",
    "    binance = ccxt.binance({\n",
    "        'rateLimit': 1200,\n",
    "        'options': {\n",
    "            'adjustForTimeDifference': True,\n",
    "        }\n",
    "    })\n",
    "\n",
    "    start_date = int(datetime.datetime(2018, 1, 1, 10, 20).timestamp() * 1000)\n",
    "    since = start_date\n",
    "\n",
    "    while len(all_candles) < limit:\n",
    "        ohlcv = binance.fetch_ohlcv(symbol, timeframe, since, min(limit, 1000))\n",
    "        \n",
    "        if len(ohlcv) == 0:\n",
    "            break\n",
    "\n",
    "        since = ohlcv[-1][0]  # Update the 'since' timestamp for the next batch\n",
    "        all_candles += ohlcv\n",
    "        time.sleep(binance.rateLimit / 1000)  # Respect the rate limit of the exchange\n",
    "\n",
    "    df = pd.DataFrame(all_candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 03:00:00</td>\n",
       "      <td>13330.26</td>\n",
       "      <td>13611.27</td>\n",
       "      <td>13290.00</td>\n",
       "      <td>13410.03</td>\n",
       "      <td>420.087030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>13434.98</td>\n",
       "      <td>13623.29</td>\n",
       "      <td>13322.15</td>\n",
       "      <td>13601.01</td>\n",
       "      <td>340.807329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 05:00:00</td>\n",
       "      <td>13615.20</td>\n",
       "      <td>13699.00</td>\n",
       "      <td>13526.50</td>\n",
       "      <td>13558.99</td>\n",
       "      <td>404.229046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>13539.00</td>\n",
       "      <td>13800.00</td>\n",
       "      <td>13510.00</td>\n",
       "      <td>13780.41</td>\n",
       "      <td>264.989684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 07:00:00</td>\n",
       "      <td>13780.00</td>\n",
       "      <td>13818.55</td>\n",
       "      <td>13555.02</td>\n",
       "      <td>13570.35</td>\n",
       "      <td>292.188777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp      open      high       low     close      volume\n",
       "0 2018-01-01 03:00:00  13330.26  13611.27  13290.00  13410.03  420.087030\n",
       "1 2018-01-01 04:00:00  13434.98  13623.29  13322.15  13601.01  340.807329\n",
       "2 2018-01-01 05:00:00  13615.20  13699.00  13526.50  13558.99  404.229046\n",
       "3 2018-01-01 06:00:00  13539.00  13800.00  13510.00  13780.41  264.989684\n",
       "4 2018-01-01 07:00:00  13780.00  13818.55  13555.02  13570.35  292.188777"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fetch_historical_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add a new column with the shifted close prices to compare with\n",
    "df['close_shifted'] = df['close'].shift(-1)\n",
    "\n",
    "# Calculate the encoded 'price_movement' based on your conditions\n",
    "df['price_movement'] = 1  # Initialize as no change\n",
    "df.loc[df['close'] < df['close_shifted'], 'price_movement'] = 2  # Price goes up\n",
    "df.loc[df['close'] > df['close_shifted'], 'price_movement'] = 0  # Price goes down\n",
    "\n",
    "# Drop the 'close_shifted' as it was only used for the calculation\n",
    "df.drop('close_shifted', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PriceMovementDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, split, data, length=10):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.data = data  # This is your encoded 'price_movement' series as a PyTorch tensor\n",
    "        self.length = length  # Sequence length for training/testing\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return 3\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.length  # Adjust length to account for sequence length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Create sequences\n",
    "        inp = self.data[idx:idx+self.length]\n",
    "        sol = self.data[idx+1:idx+self.length+1]\n",
    "\n",
    "        inp = torch.from_numpy(inp)\n",
    "        sol = torch.from_numpy(sol)\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:self.length-1] = -1\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PriceMovementDataset('train', df['price_movement'].to_numpy()[:9980])\n",
    "test_dataset = PriceMovementDataset('test', df['price_movement'].to_numpy()[9980:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price_movement'].to_numpy()[9980:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.09M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 6000\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 1.11192\n",
      "iter_dt 27.05ms; iter 100: train loss 0.52294\n",
      "iter_dt 20.00ms; iter 200: train loss 0.09539\n",
      "iter_dt 24.04ms; iter 300: train loss 0.07843\n",
      "iter_dt 19.00ms; iter 400: train loss 0.06929\n",
      "iter_dt 20.99ms; iter 500: train loss 0.06978\n",
      "iter_dt 17.00ms; iter 600: train loss 0.07865\n",
      "iter_dt 17.02ms; iter 700: train loss 0.06816\n",
      "iter_dt 12.98ms; iter 800: train loss 0.07398\n",
      "iter_dt 13.00ms; iter 900: train loss 0.06834\n",
      "iter_dt 17.00ms; iter 1000: train loss 0.07738\n",
      "iter_dt 13.00ms; iter 1100: train loss 0.07010\n",
      "iter_dt 12.00ms; iter 1200: train loss 0.06579\n",
      "iter_dt 11.99ms; iter 1300: train loss 0.07070\n",
      "iter_dt 24.00ms; iter 1400: train loss 0.06840\n",
      "iter_dt 20.99ms; iter 1500: train loss 0.06952\n",
      "iter_dt 20.00ms; iter 1600: train loss 0.07000\n",
      "iter_dt 17.00ms; iter 1700: train loss 0.07149\n",
      "iter_dt 17.00ms; iter 1800: train loss 0.08125\n",
      "iter_dt 17.00ms; iter 1900: train loss 0.06668\n",
      "iter_dt 19.00ms; iter 2000: train loss 0.07049\n",
      "iter_dt 15.00ms; iter 2100: train loss 0.07969\n",
      "iter_dt 15.00ms; iter 2200: train loss 0.07100\n",
      "iter_dt 20.00ms; iter 2300: train loss 0.06857\n",
      "iter_dt 13.00ms; iter 2400: train loss 0.06881\n",
      "iter_dt 21.00ms; iter 2500: train loss 0.06716\n",
      "iter_dt 18.02ms; iter 2600: train loss 0.06808\n",
      "iter_dt 17.00ms; iter 2700: train loss 0.06900\n",
      "iter_dt 21.00ms; iter 2800: train loss 0.06646\n",
      "iter_dt 16.00ms; iter 2900: train loss 0.06609\n",
      "iter_dt 21.00ms; iter 3000: train loss 0.07095\n",
      "iter_dt 16.00ms; iter 3100: train loss 0.06784\n",
      "iter_dt 12.00ms; iter 3200: train loss 0.06796\n",
      "iter_dt 23.00ms; iter 3300: train loss 0.07105\n",
      "iter_dt 13.00ms; iter 3400: train loss 0.06812\n",
      "iter_dt 12.00ms; iter 3500: train loss 0.07361\n",
      "iter_dt 16.00ms; iter 3600: train loss 0.06699\n",
      "iter_dt 20.98ms; iter 3700: train loss 0.07013\n",
      "iter_dt 12.00ms; iter 3800: train loss 0.07300\n",
      "iter_dt 13.00ms; iter 3900: train loss 0.06977\n",
      "iter_dt 14.00ms; iter 4000: train loss 0.06753\n",
      "iter_dt 22.04ms; iter 4100: train loss 0.07087\n",
      "iter_dt 14.00ms; iter 4200: train loss 0.06818\n",
      "iter_dt 31.98ms; iter 4300: train loss 0.07299\n",
      "iter_dt 18.00ms; iter 4400: train loss 0.06893\n",
      "iter_dt 18.00ms; iter 4500: train loss 0.06716\n",
      "iter_dt 14.97ms; iter 4600: train loss 0.06959\n",
      "iter_dt 16.98ms; iter 4700: train loss 0.07936\n",
      "iter_dt 29.00ms; iter 4800: train loss 0.07034\n",
      "iter_dt 16.00ms; iter 4900: train loss 0.06985\n",
      "iter_dt 18.00ms; iter 5000: train loss 0.07486\n",
      "iter_dt 22.98ms; iter 5100: train loss 0.06685\n",
      "iter_dt 18.00ms; iter 5200: train loss 0.06661\n",
      "iter_dt 17.00ms; iter 5300: train loss 0.06889\n",
      "iter_dt 32.96ms; iter 5400: train loss 0.06920\n",
      "iter_dt 23.00ms; iter 5500: train loss 0.07074\n",
      "iter_dt 16.98ms; iter 5600: train loss 0.06746\n",
      "iter_dt 29.00ms; iter 5700: train loss 0.07073\n",
      "iter_dt 30.00ms; iter 5800: train loss 0.07966\n",
      "iter_dt 16.00ms; iter 5900: train loss 0.06725\n"
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT claims that with [0, 2, 0, 0, 2, 0, 0, 0, 0, 2] the future price dynamics is [2, 0, 0, 2, 0, 0, 0, 0, 2, 0] but gt is [2, 0, 0, 2, 0, 0, 0, 0, 2, 2]\n",
      "GPT claims that with [2, 0, 0, 0, 0, 2, 2, 0, 2, 2] the future price dynamics is [0, 0, 0, 0, 2, 2, 0, 2, 2, 0] but gt is [0, 0, 0, 0, 2, 2, 0, 2, 2, 2]\n",
      "GPT claims that with [0, 0, 0, 0, 2, 2, 0, 2, 2, 2] the future price dynamics is [0, 0, 0, 2, 2, 0, 2, 2, 2, 0] but gt is [0, 0, 0, 2, 2, 0, 2, 2, 2, 2]\n",
      "train final score: 2769/5000 = 55.38% correct\n",
      "GPT claims that with [2, 2, 0, 0, 2, 2, 0, 2, 0, 2] the future price dynamics is [2, 0, 0, 2, 2, 0, 2, 0, 2, 2] but gt is [2, 0, 0, 2, 2, 0, 2, 0, 2, 0]\n",
      "GPT claims that with [0, 2, 0, 2, 2, 0, 2, 2, 0, 2] the future price dynamics is [2, 0, 2, 2, 0, 2, 2, 0, 2, 2] but gt is [2, 0, 2, 2, 0, 2, 2, 0, 2, 0]\n",
      "GPT claims that with [2, 0, 2, 2, 0, 2, 2, 0, 2, 0] the future price dynamics is [0, 2, 2, 0, 2, 2, 0, 2, 0, 2] but gt is [0, 2, 2, 0, 2, 2, 0, 2, 0, 1]\n",
      "test final score: 7/10 = 70.00% correct\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(3407)\n",
    "\n",
    "def eval_split(trainer, split, max_batches):\n",
    "    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
    "    n = train_dataset.length # naugy direct access shrug\n",
    "    results = []\n",
    "    mistakes_printed_already = 0\n",
    "    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        x = x.to(trainer.device)\n",
    "        y = y.to(trainer.device)\n",
    "        # isolate the input pattern alone\n",
    "        inp = x[:, :n]\n",
    "        sol = y[:, -n:]\n",
    "        # let the model sample the rest of the sequence\n",
    "        cat = model.generate(inp, n, do_sample=False) # using greedy argmax, not sampling\n",
    "        sol_candidate = cat[:, n:] # isolate the filled in sequence\n",
    "        # compare the predicted sequence to the true sequence\n",
    "        #print(\"sol: \", sol[10:15,-8:])\n",
    "        #print(\"sol_candidate: \", sol_candidate[10:15,-8:])\n",
    "        correct = (sol == sol_candidate).all(1).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\n",
    "        #print(\"correct:\",correct)\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "            if not correct[i] and mistakes_printed_already < 3: # only print up to 5 mistakes to get a sense\n",
    "                mistakes_printed_already += 1\n",
    "                print(\"GPT claims that with %s the future price dynamics is %s but gt is %s\" % (inp[i].tolist(), sol_candidate[i].tolist(), sol[i].tolist()))\n",
    "        if max_batches is not None and b+1 >= max_batches:\n",
    "            break\n",
    "    rt = torch.tensor(results, dtype=torch.float)\n",
    "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
    "    return rt.sum()\n",
    "\n",
    "# run a lot of examples from both train and test through the model and verify the output correctness\n",
    "with torch.no_grad():\n",
    "    train_score = eval_split(trainer, 'train', max_batches=50)\n",
    "    test_score  = eval_split(trainer, 'test',  max_batches=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2769.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
